{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9694239,"sourceType":"datasetVersion","datasetId":5927133}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, applications\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:29.595733Z","iopub.execute_input":"2025-04-16T11:05:29.595960Z","iopub.status.idle":"2025-04-16T11:05:46.084653Z","shell.execute_reply.started":"2025-04-16T11:05:29.595941Z","shell.execute_reply":"2025-04-16T11:05:46.083967Z"}},"outputs":[{"name":"stderr","text":"2025-04-16 11:05:31.281615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744801531.483090      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744801531.542715      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.085333Z","iopub.execute_input":"2025-04-16T11:05:46.085775Z","iopub.status.idle":"2025-04-16T11:05:46.089791Z","shell.execute_reply.started":"2025-04-16T11:05:46.085752Z","shell.execute_reply":"2025-04-16T11:05:46.088992Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# âš™ï¸ Paths\ndata_dir = Path(\"/kaggle/input/african-plums-quality-and-defect-assessment-data/african_plums_dataset/african_plums\")\ndataset_dir = Path(\"/kaggle/working/plums_improved\")\ntrain_dir = dataset_dir / \"train\"\ntest_dir = dataset_dir / \"test\"\nval_dir = dataset_dir / \"val\"\n\n# Create directories\nfor dir_path in [train_dir, test_dir, val_dir]:\n    dir_path.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.091525Z","iopub.execute_input":"2025-04-16T11:05:46.092098Z","iopub.status.idle":"2025-04-16T11:05:46.110677Z","shell.execute_reply.started":"2025-04-16T11:05:46.092076Z","shell.execute_reply":"2025-04-16T11:05:46.109742Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ğŸ” Analyze dataset\nclass_counts = {}\nfor class_dir in data_dir.iterdir():\n    if class_dir.is_dir():\n        class_name = class_dir.name\n        imgs = list(class_dir.glob(\"*.png\"))\n        class_counts[class_name] = len(imgs)\n        print(f\"Class {class_name}: {len(imgs)} images\")\n\ntotal_images = sum(class_counts.values())\nprint(f\"Total images: {total_images}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.111505Z","iopub.execute_input":"2025-04-16T11:05:46.111778Z","iopub.status.idle":"2025-04-16T11:05:46.212553Z","shell.execute_reply.started":"2025-04-16T11:05:46.111753Z","shell.execute_reply":"2025-04-16T11:05:46.212014Z"}},"outputs":[{"name":"stdout","text":"Class unripe: 826 images\nClass cracked: 162 images\nClass rotten: 720 images\nClass spotted: 759 images\nClass bruised: 319 images\nClass unaffected: 1721 images\nTotal images: 4507\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ğŸ“ Define parameters\nIMG_SIZE = (224, 224)  # Increased from 128x128 for better feature extraction\nBATCH_SIZE = 32  # Smaller batch size for better generalization\nTARGET_COUNT = 2000  # Increased target for balanced augmentation\nVAL_SPLIT = 0.15\nTEST_SPLIT = 0.15","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.213273Z","iopub.execute_input":"2025-04-16T11:05:46.213477Z","iopub.status.idle":"2025-04-16T11:05:46.217070Z","shell.execute_reply.started":"2025-04-16T11:05:46.213461Z","shell.execute_reply":"2025-04-16T11:05:46.216412Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ğŸ” Analyze dataset to find the largest class\nclass_counts = {}\nfor class_dir in data_dir.iterdir():\n    if class_dir.is_dir():\n        class_name = class_dir.name\n        imgs = list(class_dir.glob(\"*.png\"))\n        class_counts[class_name] = len(imgs)\n        print(f\"Class {class_name}: {len(imgs)} images\")\n\ntotal_images = sum(class_counts.values())\nmax_class_size = max(class_counts.values())\nprint(f\"Total images: {total_images}\")\nprint(f\"Maximum class size: {max_class_size}\")\n\n# Set target count to the size of the largest class\nTARGET_COUNT = max_class_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.217734Z","iopub.execute_input":"2025-04-16T11:05:46.217949Z","iopub.status.idle":"2025-04-16T11:05:46.243946Z","shell.execute_reply.started":"2025-04-16T11:05:46.217933Z","shell.execute_reply":"2025-04-16T11:05:46.243315Z"}},"outputs":[{"name":"stdout","text":"Class unripe: 826 images\nClass cracked: 162 images\nClass rotten: 720 images\nClass spotted: 759 images\nClass bruised: 319 images\nClass unaffected: 1721 images\nTotal images: 4507\nMaximum class size: 1721\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ğŸ”„ Advanced data augmentation (class-specific)\ndef create_augmentation_for_class(class_name):\n    \"\"\"Create class-specific augmentation generator\"\"\"\n    # Base augmentation parameters common to all classes\n    base_params = {\n        'rescale': 1./255,\n        'rotation_range': 20,\n        'width_shift_range': 0.2,\n        'height_shift_range': 0.2,\n        'shear_range': 0.15,\n        'horizontal_flip': True,\n        'vertical_flip': True,\n        'fill_mode': 'nearest'\n    }\n    \n    # Class-specific augmentation parameters\n    if class_name == 'bruised':\n        return ImageDataGenerator(\n            **base_params,\n            zoom_range=0.2,\n            brightness_range=[0.7, 1.3],\n            channel_shift_range=20\n        )\n    elif class_name == 'cracked':\n        return ImageDataGenerator(\n            **base_params,\n            zoom_range=[0.8, 1.1],  # Less aggressive zoom to preserve cracks\n            brightness_range=[0.7, 1.3]\n        )\n    elif class_name == 'rotten':\n        return ImageDataGenerator(\n            **base_params,\n            zoom_range=0.2,\n            brightness_range=[0.6, 1.2],\n            channel_shift_range=30\n        )\n    elif class_name == 'spotted':\n        return ImageDataGenerator(\n            **base_params,\n            zoom_range=0.2,\n            brightness_range=[0.7, 1.3],\n            channel_shift_range=20\n        )\n    else:  # unaffected and unripe\n        return ImageDataGenerator(\n            **base_params,\n            zoom_range=0.2,\n            brightness_range=[0.8, 1.2]\n        )\n\n# ğŸ“ Dataset preparation\ndef prepare_dataset():\n    print(\"Preparing dataset with stratified splits...\")\n    \n    # Create directories for each class\n    for split_dir in [train_dir, val_dir, test_dir]:\n        for class_name in class_counts.keys():\n            (split_dir / class_name).mkdir(parents=True, exist_ok=True)\n    \n    # Process each class\n    for class_dir in data_dir.iterdir():\n        if not class_dir.is_dir():\n            continue\n            \n        class_name = class_dir.name\n        images = list(class_dir.glob(\"*.png\"))\n        random.shuffle(images)\n        \n        # Determine split sizes\n        val_size = int(len(images) * VAL_SPLIT)\n        test_size = int(len(images) * TEST_SPLIT)\n        \n        # Split the data\n        val_images = images[:val_size]\n        test_images = images[val_size:val_size+test_size]\n        train_images = images[val_size+test_size:]\n        \n        print(f\"Class {class_name}: {len(train_images)} train, {len(val_images)} val, {len(test_images)} test\")\n        \n        # Copy images to respective directories\n        for split_name, img_list, output_dir in [\n            (\"train\", train_images, train_dir),\n            (\"val\", val_images, val_dir),\n            (\"test\", test_images, test_dir)\n        ]:\n            for img_path in tqdm(img_list, desc=f\"Copying {split_name} {class_name}\", leave=False):\n                output_path = output_dir / class_name / img_path.name\n                shutil.copy(img_path, output_path)\n        \n        # Augment minority classes to TARGET_COUNT\n        if len(train_images) < TARGET_COUNT:\n            print(f\"Augmenting class '{class_name}' from {len(train_images)} to {TARGET_COUNT}\")\n            augmentor = create_augmentation_for_class(class_name)\n            \n            needed = TARGET_COUNT - len(train_images)\n            aug_count = 0\n            \n            # Create batches for augmentation to speed up process\n            batch_size = min(100, len(train_images))\n            img_paths = list(train_images) * (needed // len(train_images) + 1)\n            img_paths = img_paths[:needed]\n            \n            # Process in batches\n            for i in range(0, len(img_paths), batch_size):\n                batch_paths = img_paths[i:i+batch_size]\n                for j, img_path in enumerate(batch_paths):\n                    try:\n                        # Load image\n                        img = cv2.imread(str(img_path))\n                        if img is None:\n                            continue\n                            \n                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                        img = img.astype(np.float32) / 255.0\n                        img = np.expand_dims(img, 0)\n                        \n                        # Generate augmented image\n                        aug_img = next(augmentor.flow(img, batch_size=1))[0]\n                        aug_img = (aug_img * 255).astype(np.uint8)\n                        \n                        # Save augmented image\n                        output_path = train_dir / class_name / f\"{img_path.stem}_aug_{aug_count}.png\"\n                        cv2.imwrite(str(output_path), cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR))\n                        aug_count += 1\n                    except Exception as e:\n                        print(f\"Error augmenting {img_path}: {str(e)}\")\n                        continue\n                \n                print(f\"  Progress: {aug_count}/{needed} augmented images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.244585Z","iopub.execute_input":"2025-04-16T11:05:46.244817Z","iopub.status.idle":"2025-04-16T11:05:46.257144Z","shell.execute_reply.started":"2025-04-16T11:05:46.244801Z","shell.execute_reply":"2025-04-16T11:05:46.256431Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Run dataset preparation\nprepare_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:46.257768Z","iopub.execute_input":"2025-04-16T11:05:46.257967Z","iopub.status.idle":"2025-04-16T11:22:33.470728Z","shell.execute_reply.started":"2025-04-16T11:05:46.257951Z","shell.execute_reply":"2025-04-16T11:22:33.470025Z"}},"outputs":[{"name":"stdout","text":"Preparing dataset with stratified splits...\nClass unripe: 580 train, 123 val, 123 test\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Augmenting class 'unripe' from 580 to 1721\n  Progress: 100/1141 augmented images\n  Progress: 200/1141 augmented images\n  Progress: 300/1141 augmented images\n  Progress: 400/1141 augmented images\n  Progress: 500/1141 augmented images\n  Progress: 600/1141 augmented images\n  Progress: 700/1141 augmented images\n  Progress: 800/1141 augmented images\n  Progress: 900/1141 augmented images\n  Progress: 1000/1141 augmented images\n  Progress: 1100/1141 augmented images\n  Progress: 1141/1141 augmented images\nClass cracked: 114 train, 24 val, 24 test\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Augmenting class 'cracked' from 114 to 1721\n  Progress: 100/1607 augmented images\n  Progress: 200/1607 augmented images\n  Progress: 300/1607 augmented images\n  Progress: 400/1607 augmented images\n  Progress: 500/1607 augmented images\n  Progress: 600/1607 augmented images\n  Progress: 700/1607 augmented images\n  Progress: 800/1607 augmented images\n  Progress: 900/1607 augmented images\n  Progress: 1000/1607 augmented images\n  Progress: 1100/1607 augmented images\n  Progress: 1200/1607 augmented images\n  Progress: 1300/1607 augmented images\n  Progress: 1400/1607 augmented images\n  Progress: 1500/1607 augmented images\n  Progress: 1600/1607 augmented images\n  Progress: 1607/1607 augmented images\nClass rotten: 504 train, 108 val, 108 test\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Augmenting class 'rotten' from 504 to 1721\n  Progress: 100/1217 augmented images\n  Progress: 200/1217 augmented images\n  Progress: 300/1217 augmented images\n  Progress: 400/1217 augmented images\n  Progress: 500/1217 augmented images\n  Progress: 600/1217 augmented images\n  Progress: 700/1217 augmented images\n  Progress: 800/1217 augmented images\n  Progress: 900/1217 augmented images\n  Progress: 1000/1217 augmented images\n  Progress: 1100/1217 augmented images\n  Progress: 1200/1217 augmented images\n  Progress: 1217/1217 augmented images\nClass spotted: 533 train, 113 val, 113 test\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Augmenting class 'spotted' from 533 to 1721\n  Progress: 100/1188 augmented images\n  Progress: 200/1188 augmented images\n  Progress: 300/1188 augmented images\n  Progress: 400/1188 augmented images\n  Progress: 500/1188 augmented images\n  Progress: 600/1188 augmented images\n  Progress: 700/1188 augmented images\n  Progress: 800/1188 augmented images\n  Progress: 900/1188 augmented images\n  Progress: 1000/1188 augmented images\n  Progress: 1100/1188 augmented images\n  Progress: 1188/1188 augmented images\nClass bruised: 225 train, 47 val, 47 test\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Augmenting class 'bruised' from 225 to 1721\n  Progress: 100/1496 augmented images\n  Progress: 200/1496 augmented images\n  Progress: 300/1496 augmented images\n  Progress: 400/1496 augmented images\n  Progress: 500/1496 augmented images\n  Progress: 600/1496 augmented images\n  Progress: 700/1496 augmented images\n  Progress: 800/1496 augmented images\n  Progress: 900/1496 augmented images\n  Progress: 1000/1496 augmented images\n  Progress: 1100/1496 augmented images\n  Progress: 1200/1496 augmented images\n  Progress: 1300/1496 augmented images\n  Progress: 1400/1496 augmented images\n  Progress: 1496/1496 augmented images\nClass unaffected: 1205 train, 258 val, 258 test\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Augmenting class 'unaffected' from 1205 to 1721\n  Progress: 100/516 augmented images\n  Progress: 200/516 augmented images\n  Progress: 300/516 augmented images\n  Progress: 400/516 augmented images\n  Progress: 500/516 augmented images\n  Progress: 516/516 augmented images\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ğŸ“ Dataset preparation with stratified split and preprocessing\nprint(\"Preparing dataset with preprocessing and stratified splits...\")\nclass_data = {}\n\n# First, collect all images by class\nfor class_dir in data_dir.iterdir():\n    if class_dir.is_dir():\n        class_name = class_dir.name\n        class_data[class_name] = list(class_dir.glob(\"*.png\"))\n        print(f\"Found {len(class_data[class_name])} images for '{class_name}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:22:33.473178Z","iopub.execute_input":"2025-04-16T11:22:33.473437Z","iopub.status.idle":"2025-04-16T11:22:33.513805Z","shell.execute_reply.started":"2025-04-16T11:22:33.473420Z","shell.execute_reply":"2025-04-16T11:22:33.513228Z"}},"outputs":[{"name":"stdout","text":"Preparing dataset with preprocessing and stratified splits...\nFound 826 images for 'unripe'\nFound 162 images for 'cracked'\nFound 720 images for 'rotten'\nFound 759 images for 'spotted'\nFound 319 images for 'bruised'\nFound 1721 images for 'unaffected'\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ğŸ” Data generators for model training\nprint(\"Creating data generators...\")\n\n# Training generator with augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2],\n    fill_mode='nearest'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:22:33.514412Z","iopub.execute_input":"2025-04-16T11:22:33.514612Z","iopub.status.idle":"2025-04-16T11:22:33.519034Z","shell.execute_reply.started":"2025-04-16T11:22:33.514588Z","shell.execute_reply":"2025-04-16T11:22:33.518455Z"}},"outputs":[{"name":"stdout","text":"Creating data generators...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Evaluation generators without augmentation\neval_datagen = ImageDataGenerator(rescale=1./255)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:22:33.519815Z","iopub.execute_input":"2025-04-16T11:22:33.520029Z","iopub.status.idle":"2025-04-16T11:22:33.532957Z","shell.execute_reply.started":"2025-04-16T11:22:33.520004Z","shell.execute_reply":"2025-04-16T11:22:33.532362Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Create generators\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True,\n    seed=SEED\n)\n\nval_generator = eval_datagen.flow_from_directory(\n    val_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\ntest_generator = eval_datagen.flow_from_directory(\n    test_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:22:33.533576Z","iopub.execute_input":"2025-04-16T11:22:33.533791Z","iopub.status.idle":"2025-04-16T11:22:33.705522Z","shell.execute_reply.started":"2025-04-16T11:22:33.533775Z","shell.execute_reply":"2025-04-16T11:22:33.705012Z"}},"outputs":[{"name":"stdout","text":"Found 10326 images belonging to 6 classes.\nFound 673 images belonging to 6 classes.\nFound 673 images belonging to 6 classes.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ğŸ·ï¸ Class mapping\nclass_indices = train_generator.class_indices\nprint(\"Class mapping:\", class_indices)\nclass_names = list(class_indices.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:22:33.706198Z","iopub.execute_input":"2025-04-16T11:22:33.706446Z","iopub.status.idle":"2025-04-16T11:22:33.710240Z","shell.execute_reply.started":"2025-04-16T11:22:33.706425Z","shell.execute_reply":"2025-04-16T11:22:33.709724Z"}},"outputs":[{"name":"stdout","text":"Class mapping: {'bruised': 0, 'cracked': 1, 'rotten': 2, 'spotted': 3, 'unaffected': 4, 'unripe': 5}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ğŸ§  Model definition - EfficientNetB1 with custom head\ndef build_model(input_shape=(224, 224, 3), num_classes=6):\n    # Use EfficientNetB1 as base model (slightly better than B0)\n    base_model = applications.EfficientNetB1(\n        include_top=False,\n        weights='imagenet',\n        input_shape=input_shape\n    )\n    \n    # Freeze early layers\n    for layer in base_model.layers[:-30]:\n        layer.trainable = False\n    \n    # Create model\n    inputs = keras.Input(shape=input_shape)\n    x = base_model(inputs, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Add dropout and regularization\n    x = layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4))(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4))(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:22:33.710896Z","iopub.execute_input":"2025-04-16T11:22:33.711139Z","iopub.status.idle":"2025-04-16T11:22:33.727402Z","shell.execute_reply.started":"2025-04-16T11:22:33.711118Z","shell.execute_reply":"2025-04-16T11:22:33.726722Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = build_model(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), num_classes=len(class_names))\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:22:33.728037Z","iopub.execute_input":"2025-04-16T11:22:33.728228Z","iopub.status.idle":"2025-04-16T11:22:37.894462Z","shell.execute_reply.started":"2025-04-16T11:22:33.728208Z","shell.execute_reply":"2025-04-16T11:22:37.893791Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1744802554.718200      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n\u001b[1m27018416/27018416\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ efficientnetb1 (\u001b[38;5;33mFunctional\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          â”‚       \u001b[38;5;34m6,575,239\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling2d             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                â”‚           \u001b[38;5;34m5,120\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 â”‚         \u001b[38;5;34m655,872\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚         \u001b[38;5;34m131,328\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   â”‚           \u001b[38;5;34m1,542\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ efficientnetb1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,575,239</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling2d             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,542</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,369,101\u001b[0m (28.11 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,369,101</span> (28.11 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,263,110\u001b[0m (12.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,263,110</span> (12.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,105,991\u001b[0m (15.66 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,105,991</span> (15.66 MB)\n</pre>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# ğŸ“… Training parameters\nEPOCHS = 100\nSTEP_SIZE_TRAIN = train_generator.samples // train_generator.batch_size\nSTEP_SIZE_VAL = val_generator.samples // val_generator.batch_size or 1\n\n# ğŸ¯ Compute class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_generator.classes),\n    y=train_generator.classes\n)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(\"Class weights:\", class_weights_dict)\n\n# ğŸ’¾ Callbacks\ncheckpoint = keras.callbacks.ModelCheckpoint(\n    'best_plum_model.keras',\n    monitor='val_loss',\n    save_best_only=True,\n    mode='min',\n    verbose=1\n)\n\nearly_stop = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True,\n    verbose=1\n)\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-7,\n    verbose=1\n)\n\n# ğŸš€ Compile model\noptimizer = keras.optimizers.AdamW(\n    learning_rate=1e-4,\n    weight_decay=1e-5,\n    clipnorm=1.0  # Gradient clipping\n)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:22:37.895207Z","iopub.execute_input":"2025-04-16T11:22:37.895407Z","iopub.status.idle":"2025-04-16T11:22:37.912447Z","shell.execute_reply.started":"2025-04-16T11:22:37.895391Z","shell.execute_reply":"2025-04-16T11:22:37.911821Z"}},"outputs":[{"name":"stdout","text":"Class weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ğŸ‹ï¸â€â™‚ï¸ Train model\nprint(\"Training model...\")\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=STEP_SIZE_TRAIN,\n    epochs=25,\n    validation_data=val_generator,\n    validation_steps=STEP_SIZE_VAL,\n    callbacks=[checkpoint, early_stop, reduce_lr],\n    class_weight=class_weights_dict,\n    verbose=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:26:47.155300Z","iopub.execute_input":"2025-04-16T11:26:47.156036Z","iopub.status.idle":"2025-04-16T11:47:08.937887Z","shell.execute_reply.started":"2025-04-16T11:26:47.156011Z","shell.execute_reply":"2025-04-16T11:47:08.936734Z"}},"outputs":[{"name":"stdout","text":"Training model...\nEpoch 1/25\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473ms/step - accuracy: 0.1692 - loss: 2.1835\nEpoch 1: val_loss improved from inf to 1.89015, saving model to best_plum_model.keras\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 514ms/step - accuracy: 0.1693 - loss: 2.1833 - val_accuracy: 0.1815 - val_loss: 1.8902 - learning_rate: 1.0000e-04\nEpoch 2/25\n\u001b[1m  1/322\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m15s\u001b[0m 47ms/step - accuracy: 0.0625 - loss: 2.1213","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2: val_loss improved from 1.89015 to 1.73671, saving model to best_plum_model.keras\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.0625 - loss: 2.1213 - val_accuracy: 1.0000 - val_loss: 1.7367 - learning_rate: 1.0000e-04\nEpoch 3/25\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - accuracy: 0.1642 - loss: 2.0337\nEpoch 3: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 485ms/step - accuracy: 0.1642 - loss: 2.0336 - val_accuracy: 0.1815 - val_loss: 1.8693 - learning_rate: 1.0000e-04\nEpoch 4/25\n\u001b[1m  1/322\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.1562 - loss: 1.9992\nEpoch 4: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130us/step - accuracy: 0.1562 - loss: 1.9992 - val_accuracy: 1.0000 - val_loss: 1.8079 - learning_rate: 1.0000e-04\nEpoch 5/25\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - accuracy: 0.1723 - loss: 1.9703\nEpoch 5: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 485ms/step - accuracy: 0.1723 - loss: 1.9703 - val_accuracy: 0.0357 - val_loss: 1.8966 - learning_rate: 1.0000e-04\nEpoch 6/25\n\u001b[1m  1/322\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.1250 - loss: 1.9402\nEpoch 6: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158us/step - accuracy: 0.1250 - loss: 1.9402 - val_accuracy: 0.0000e+00 - val_loss: 1.8958 - learning_rate: 1.0000e-04\nEpoch 7/25\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step - accuracy: 0.1672 - loss: 1.9477\nEpoch 7: val_loss did not improve from 1.73671\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 486ms/step - accuracy: 0.1672 - loss: 1.9477 - val_accuracy: 0.1815 - val_loss: 1.8964 - learning_rate: 1.0000e-04\nEpoch 8/25\n\u001b[1m  1/322\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.1562 - loss: 1.9598\nEpoch 8: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144us/step - accuracy: 0.1562 - loss: 1.9598 - val_accuracy: 1.0000 - val_loss: 1.8805 - learning_rate: 5.0000e-05\nEpoch 9/25\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474ms/step - accuracy: 0.1628 - loss: 1.9176\nEpoch 9: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 483ms/step - accuracy: 0.1628 - loss: 1.9176 - val_accuracy: 0.0699 - val_loss: 1.8977 - learning_rate: 5.0000e-05\nEpoch 10/25\n\u001b[1m  1/322\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.1250 - loss: 1.9308\nEpoch 10: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138us/step - accuracy: 0.1250 - loss: 1.9308 - val_accuracy: 0.0000e+00 - val_loss: 1.8962 - learning_rate: 5.0000e-05\nEpoch 11/25\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.1643 - loss: 1.9089\nEpoch 11: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 480ms/step - accuracy: 0.1643 - loss: 1.9089 - val_accuracy: 0.0699 - val_loss: 1.8973 - learning_rate: 5.0000e-05\nEpoch 12/25\n\u001b[1m  1/322\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.1250 - loss: 1.9281\nEpoch 12: val_loss did not improve from 1.73671\n\nEpoch 12: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148us/step - accuracy: 0.1250 - loss: 1.9281 - val_accuracy: 0.0000e+00 - val_loss: 1.8961 - learning_rate: 5.0000e-05\nEpoch 13/25\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - accuracy: 0.1786 - loss: 1.9055\nEpoch 13: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 486ms/step - accuracy: 0.1785 - loss: 1.9055 - val_accuracy: 0.0699 - val_loss: 1.8970 - learning_rate: 2.5000e-05\nEpoch 14/25\n\u001b[1m  1/322\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.1562 - loss: 1.9083\nEpoch 14: val_loss did not improve from 1.73671\n\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125us/step - accuracy: 0.1562 - loss: 1.9083 - val_accuracy: 0.0000e+00 - val_loss: 1.8961 - learning_rate: 2.5000e-05\nEpoch 15/25\n\u001b[1m179/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:08\u001b[0m 480ms/step - accuracy: 0.1691 - loss: 1.9044","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2060498334.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ğŸ‹ï¸â€â™‚ï¸ Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"# ğŸ‹ï¸â€â™‚ï¸ Train model\nprint(\"Training model...\")\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=STEP_SIZE_TRAIN,\n    epochs=EPOCHS,\n    validation_data=val_generator,\n    validation_steps=STEP_SIZE_VAL,\n    callbacks=[checkpoint, early_stop, reduce_lr],\n    class_weight=class_weights_dict,\n    verbose=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:25:25.505419Z","iopub.status.idle":"2025-04-16T11:25:25.505794Z","shell.execute_reply.started":"2025-04-16T11:25:25.505601Z","shell.execute_reply":"2025-04-16T11:25:25.505616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ğŸ“Š Plot training history\ndef plot_history(history):\n    plt.figure(figsize=(12, 5))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:25:25.507191Z","iopub.status.idle":"2025-04-16T11:25:25.507470Z","shell.execute_reply.started":"2025-04-16T11:25:25.507336Z","shell.execute_reply":"2025-04-16T11:25:25.507348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_history(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:25:25.509102Z","iopub.status.idle":"2025-04-16T11:25:25.509416Z","shell.execute_reply.started":"2025-04-16T11:25:25.509258Z","shell.execute_reply":"2025-04-16T11:25:25.509272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ğŸ” Model evaluation\nprint(\"Evaluating model on test set...\")\n# Load best model\nbest_model = keras.models.load_model('best_plum_model.keras')\n\n# Evaluate on test set\ntest_loss, test_acc = best_model.evaluate(test_generator)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# Get predictions\ny_pred = best_model.predict(test_generator)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = test_generator.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:25:25.511766Z","iopub.status.idle":"2025-04-16T11:25:25.512071Z","shell.execute_reply.started":"2025-04-16T11:25:25.511921Z","shell.execute_reply":"2025-04-16T11:25:25.511934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n           xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:25:25.513183Z","iopub.status.idle":"2025-04-16T11:25:25.513466Z","shell.execute_reply.started":"2025-04-16T11:25:25.513328Z","shell.execute_reply":"2025-04-16T11:25:25.513340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classification report\nreport = classification_report(y_true, y_pred_classes, target_names=class_names, output_dict=True)\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=class_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:25:25.514329Z","iopub.status.idle":"2025-04-16T11:25:25.514597Z","shell.execute_reply.started":"2025-04-16T11:25:25.514461Z","shell.execute_reply":"2025-04-16T11:25:25.514473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot per-class metrics\nreport_df = pd.DataFrame(report).transpose()\nplt.figure(figsize=(12, 8))\nsns.heatmap(report_df.iloc[:-3, :-1], annot=True, cmap='YlGnBu', fmt='.2f')\nplt.title('Per-Class Performance Metrics')\nplt.tight_layout()\nplt.savefig('class_metrics.png')\nplt.show()\n\nprint(\"Training and evaluation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:25:25.515340Z","iopub.status.idle":"2025-04-16T11:25:25.515670Z","shell.execute_reply.started":"2025-04-16T11:25:25.515483Z","shell.execute_reply":"2025-04-16T11:25:25.515500Z"}},"outputs":[],"execution_count":null}]}